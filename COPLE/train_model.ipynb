{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/joao/.local/lib/python3.10/site-packages (4.24.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/joao/.local/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/lib/python3.10/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/joao/.local/lib/python3.10/site-packages (from transformers) (0.11.0)\n",
      "Requirement already satisfied: filelock in /home/joao/.local/lib/python3.10/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/joao/.local/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/joao/.local/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: requests in /usr/lib/python3.10/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3.10/site-packages (from requests->transformers) (1.26.13)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/joao/.local/lib/python3.10/site-packages (2.7.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /home/joao/.local/lib/python3.10/site-packages (from datasets) (0.11.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/lib/python3.10/site-packages (from datasets) (2.28.2)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3.10/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/joao/.local/lib/python3.10/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: dill<0.3.7 in /home/joao/.local/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: responses<0.19 in /home/joao/.local/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: xxhash in /home/joao/.local/lib/python3.10/site-packages (from datasets) (3.1.0)\n",
      "Requirement already satisfied: multiprocess in /home/joao/.local/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/joao/.local/lib/python3.10/site-packages (from datasets) (10.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/lib/python3.10/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: pandas in /home/joao/.local/lib/python3.10/site-packages (from datasets) (1.5.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/joao/.local/lib/python3.10/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/joao/.local/lib/python3.10/site-packages (from datasets) (2022.11.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3.10/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/joao/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.3)\n",
      "Requirement already satisfied: filelock in /home/joao/.local/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3.10/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: evaluate in /home/joao/.local/lib/python3.10/site-packages (0.3.0)\n",
      "Requirement already satisfied: multiprocess in /home/joao/.local/lib/python3.10/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/joao/.local/lib/python3.10/site-packages (from evaluate) (0.11.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/lib/python3.10/site-packages (from evaluate) (2.28.2)\n",
      "Requirement already satisfied: xxhash in /home/joao/.local/lib/python3.10/site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/joao/.local/lib/python3.10/site-packages (from evaluate) (2022.11.0)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3.10/site-packages (from evaluate) (23.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/joao/.local/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/joao/.local/lib/python3.10/site-packages (from evaluate) (2.7.0)\n",
      "Requirement already satisfied: pandas in /home/joao/.local/lib/python3.10/site-packages (from evaluate) (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/joao/.local/lib/python3.10/site-packages (from evaluate) (1.23.5)\n",
      "Requirement already satisfied: dill in /home/joao/.local/lib/python3.10/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/joao/.local/lib/python3.10/site-packages (from evaluate) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /usr/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/joao/.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (10.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: filelock in /home/joao/.local/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3.10/site-packages (from pandas->evaluate) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/joao/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-11 16:01:06.777602: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-11 16:01:07.695820: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-11 16:01:07.695873: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-11 16:01:07.695889: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, get_scheduler\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-697819366a407adc\n",
      "Found cached dataset csv (/home/joao/.cache/huggingface/datasets/csv/default-697819366a407adc/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "Using custom data configuration default-697819366a407adc\n",
      "Found cached dataset csv (/home/joao/.cache/huggingface/datasets/csv/default-697819366a407adc/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "Using custom data configuration default-697819366a407adc\n",
      "Found cached dataset csv (/home/joao/.cache/huggingface/datasets/csv/default-697819366a407adc/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A2', 'B1', 'B2', 'A1', 'C1']\n"
     ]
    }
   ],
   "source": [
    "raw_train_dataset = load_dataset('csv', data_files='cople_ortho.csv', split='train[:80%]')\n",
    "raw_validation_dataset = load_dataset('csv', data_files='cople_ortho.csv', split='train[80%:90%]')\n",
    "raw_test_dataset = load_dataset('csv', data_files='cople_ortho.csv', split='train[90%:]')\n",
    "labels = pd.read_csv('cople_ortho.csv')['Proficiency'].unique().tolist()\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'neuralmind/bert-base-portuguese-cased'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=len(labels))\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"model_4_acc86%\")\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Id', 'Proficiency', 'Text'],\n",
      "    num_rows: 816\n",
      "})\n",
      "Dataset({\n",
      "    features: ['Id', 'Proficiency', 'Text'],\n",
      "    num_rows: 102\n",
      "})\n",
      "Dataset({\n",
      "    features: ['Id', 'Proficiency', 'Text'],\n",
      "    num_rows: 102\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(raw_train_dataset)\n",
    "print(raw_validation_dataset)\n",
    "print(raw_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataframe:              Id Proficiency                                               Text\n",
      "0  pl006CVETI_2          A2  Bom dia,\\n\\neu queria reclamar [Fig1] o meu te...\n",
      "1    es019CAETF          A2  Eu gostaria de fazer uma reunião de amigos num...\n",
      "2    de007CVMTD          B1  Olá, Nuno,\\n\\nAs minhas férias são um espectác...\n",
      "3    de016CVATF          B2  Durante um debate ou uma simples conversa num ...\n",
      "4    it026CAETF          A2  Normalmente não faço muitos planos para o ano ...\n",
      "Validation dataframe:                Id Proficiency  \\\n",
      "816    fr024CVETI          A2   \n",
      "817    en037CVETI          A2   \n",
      "818    it001CVETF          A2   \n",
      "819  ru007CAATF_2          B2   \n",
      "820  zh031CAMTD_2          B1   \n",
      "\n",
      "                                                  Text  \n",
      "816  Lisboa, 19.09.2011\\n\\nQuerida FF,\\n\\nFaz 3 sem...  \n",
      "817  Quando Roberto chegou a casa ontem, estava mui...  \n",
      "818  Infelizmente, em Itália temos muitos problemas...  \n",
      "819  Sebastianismo: o rei D. Sebastião, \" Desejado\"...  \n",
      "820  Quando vivemos num país estrangeiro, devemos c...  \n",
      "Test dataframe:                Id Proficiency  \\\n",
      "918    es007CVMTD          B1   \n",
      "919  ru022CVMTF_2          B1   \n",
      "920    fr034CVMTF          B1   \n",
      "921    de037CVSTF          C1   \n",
      "922    de013CVITI          A1   \n",
      "\n",
      "                                                  Text  \n",
      "918  Querido Nuno:\\n\\nGostaria de te contar como é ...  \n",
      "919  Eu totalmente concordo com as opiniões destes ...  \n",
      "920  Acho que esta afirmação está bastante verdadei...  \n",
      "921  Nesta citação, Isabel Leal identifica a necess...  \n",
      "922  Normalmente, eu levanto-me às sete e um quarto...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cople_ortho.csv\")\n",
    "training_df = df[:816]\n",
    "validation_df = df[816:816+102]\n",
    "test_df = df[-102:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total class distribution:\n",
      "Proficiency\n",
      "A1     92\n",
      "A2    399\n",
      "B1    292\n",
      "B2    199\n",
      "C1     38\n",
      "dtype: int64\n",
      "Train class distribution:\n",
      "Proficiency\n",
      "A1     77\n",
      "A2    315\n",
      "B1    236\n",
      "B2    160\n",
      "C1     28\n",
      "dtype: int64\n",
      "Validation class distribution:\n",
      "Proficiency\n",
      "A1     7\n",
      "A2    43\n",
      "B1    27\n",
      "B2    18\n",
      "C1     7\n",
      "dtype: int64\n",
      "Test class distribution:\n",
      "Proficiency\n",
      "A1     8\n",
      "A2    41\n",
      "B1    29\n",
      "B2    21\n",
      "C1     3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total class distribution:\\n{df.value_counts('Proficiency', sort=False)}\")\n",
    "print(f\"Train class distribution:\\n{training_df.value_counts('Proficiency', sort=False)}\")\n",
    "print(f\"Validation class distribution:\\n{validation_df.value_counts('Proficiency', sort=False)}\")\n",
    "print(f\"Test class distribution:\\n{test_df.value_counts('Proficiency', sort=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_string_to_int(s):\n",
    "    if s == 'A1':\n",
    "        return 0\n",
    "    if s == 'A2':\n",
    "        return 1\n",
    "    if s == 'B1':\n",
    "        return 2\n",
    "    if s == 'B2':\n",
    "        return 3\n",
    "    if s == 'C1':\n",
    "        return 4\n",
    "    if s == 'C2':\n",
    "        return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/joao/.cache/huggingface/datasets/csv/default-697819366a407adc/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-647d7d12b977d667.arrow\n",
      "Loading cached processed dataset at /home/joao/.cache/huggingface/datasets/csv/default-697819366a407adc/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-993bbb55c8e2030c.arrow\n",
      "Loading cached processed dataset at /home/joao/.cache/huggingface/datasets/csv/default-697819366a407adc/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0cb2513695af1ffd.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    example['labels'] = [class_string_to_int(proficiency) for proficiency in example['Proficiency']]\n",
    "    return tokenizer(example[\"Text\"], truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "tokenized_train_datasets = raw_train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_validation_datasets = raw_validation_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_datasets = raw_test_dataset.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Id', 'Proficiency', 'Text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 816\n",
      "})\n",
      "Dataset({\n",
      "    features: ['Id', 'Proficiency', 'Text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 102\n",
      "})\n",
      "Dataset({\n",
      "    features: ['Id', 'Proficiency', 'Text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 102\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_datasets)\n",
    "print(tokenized_validation_datasets)\n",
    "print(tokenized_test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_datasets = tokenized_train_datasets.remove_columns([\"Text\", \"Id\", \"Proficiency\"])\n",
    "tokenized_train_datasets.set_format(\"torch\")\n",
    "tokenized_train_datasets.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_validation_datasets = tokenized_validation_datasets.remove_columns([\"Text\", \"Id\", \"Proficiency\"])\n",
    "tokenized_validation_datasets.set_format(\"torch\")\n",
    "tokenized_validation_datasets.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test_datasets = tokenized_test_datasets.remove_columns([\"Text\", \"Id\", \"Proficiency\"])\n",
    "tokenized_test_datasets.set_format(\"torch\")\n",
    "tokenized_test_datasets.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_train_datasets, shuffle=True, batch_size=1, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_validation_datasets, batch_size=1, collate_fn=data_collator\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_test_datasets, batch_size=1, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'labels': torch.Size([1]),\n",
       " 'input_ids': torch.Size([1, 186]),\n",
       " 'token_type_ids': torch.Size([1, 186]),\n",
       " 'attention_mask': torch.Size([1, 186])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0102, grad_fn=<NllLossBackward0>) torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load('accuracy')\n",
    "recall = evaluate.load('recall')\n",
    "precision = evaluate.load('precision')\n",
    "f1 = evaluate.load('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "train_loss_df = []\n",
    "valid_loss_df = []\n",
    "patience = 10\n",
    "min_delta = 0.5\n",
    "counter = 0\n",
    "min_valid_loss = np.inf\n",
    "\n",
    "if hasattr(torch.cuda, 'empty_cache'):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        if hasattr(torch.cuda, 'empty_cache'):\n",
    "            torch.cuda.empty_cache()\n",
    "    train_loss /= len(train_dataloader)\n",
    "    valid_loss = 0.0\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        valid_loss += loss.item()\n",
    "        if hasattr(torch.cuda, 'empty_cache'):\n",
    "            torch.cuda.empty_cache()\n",
    "    valid_loss /= len(eval_dataloader)\n",
    "    train_loss_df.append(train_loss)\n",
    "    valid_loss_df.append(valid_loss)\n",
    "    if valid_loss < min_valid_loss:\n",
    "        print(f\"Epoch {epoch} - Train loss: {train_loss} - Validation loss {valid_loss}\")\n",
    "        min_valid_loss = valid_loss\n",
    "        counter = 0\n",
    "        model_path = 'model_{}'.format(epoch)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    elif valid_loss > (min_valid_loss + min_delta):\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping:\\nEpoch {epoch} - Train loss: {train_loss} - Validation loss {valid_loss}\")\n",
    "            break\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_loss_df)\n",
    "print(valid_loss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8627450980392157}\n",
      "{'recall': 0.8627450980392157}\n",
      "{'precision': 0.8792557932263815}\n",
      "{'f1': 0.8640753147118625}\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate.load('accuracy')\n",
    "recall = evaluate.load('recall')\n",
    "precision = evaluate.load('precision')\n",
    "f1 = evaluate.load('f1')\n",
    "preds = []\n",
    "model.eval()\n",
    "for batch in test_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    preds.append(predictions)\n",
    "    accuracy.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    recall.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    precision.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    f1.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "print(accuracy.compute())\n",
    "print(recall.compute(average=\"weighted\"))\n",
    "print(precision.compute(average=\"weighted\"))\n",
    "print(f1.compute(average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = [x.item() for x in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_df = test_df\n",
    "# preds_df[\"labels\"] = class_string_to_int()...\n",
    "# preds_df.drop(['Proficiency', 'Text'], axis=1, inplace=True)\n",
    "# preds_df[\"predictions\"] = preds\n",
    "# preds_df.to_csv(\"best_model_preds_last_10%.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./third_model_1000_epochs\"\n",
    "torch.save(model.state_dict(), path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
